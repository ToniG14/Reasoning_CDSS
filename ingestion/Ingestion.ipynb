{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Requiered Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.services.ingestion_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF to Markdown conversion (Docling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "pdf_path = \"\"  # Replace with the path to your PDF file\n",
    "markdown_path = ''  # Replace with the desired output path\n",
    "convert_pdf_to_markdown(pdf_path, markdown_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "pdf_path = \"\"  # Replace with the path to your PDF file\n",
    "markdown_path = ''  # Replace with the desired output path\n",
    "convert_pdf_to_markdown(pdf_path, markdown_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the documents form the Markdown files (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define headers to split on and their corresponding metadata keys\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Title\"),\n",
    "    (\"##\", \"Section\"),\n",
    "    (\"###\", \"Subsection\"),\n",
    "    (\"####\", \"Figure/Table/SupplementaryTable\")\n",
    "]\n",
    "\n",
    "# Initialize the Markdown header splitter with the specified headers\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_folder = \"\"\n",
    "markdown_documents = read_markdown_files(markdown_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks = []\n",
    "for doc in markdown_documents:\n",
    "    chunks = markdown_splitter.split_text(doc[\"content\"])\n",
    "    # Add unique ID and file name as metadata to each chunk for traceability\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"source_file\"] = doc[\"file_name\"]\n",
    "        chunk.metadata[\"id\"] = str(uuid.uuid4())  # Generate a unique ID for each chunk\n",
    "        processed_chunks.append(chunk)\n",
    "\n",
    "# Each 'chunk' is a Document object with 'page_content' and 'metadata'\n",
    "for chunk in processed_chunks:\n",
    "    print(f\"Chunk ID: {chunk.metadata['id']}\")\n",
    "    print(f\"Source File: {chunk.metadata['source_file']}\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")\n",
    "    print(f\"Content: {chunk.page_content[:100]}...\")  # Print the first 100 characters of the content\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update chunks' metadata with cross-references\n",
    "for chunk in processed_chunks:\n",
    "    references = find_references(chunk.page_content)\n",
    "    if references:\n",
    "        chunk.metadata['References'] = references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_chunks = remove_duplicate_references(processed_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")  # Replace with your model\n",
    "\n",
    "# Prepare documents for storage\n",
    "documents = [chunk for chunk in cleaned_chunks]\n",
    "ids = [chunk.metadata[\"id\"] for chunk in cleaned_chunks]  # Ensure each chunk has a unique ID\n",
    "\n",
    "# Define collection and persistence directory\n",
    "collection_name = \"\" # Replace with your desired collection name\n",
    "persist_directory = \"\" # Replace with your desired directory for persistence\n",
    "\n",
    "# Create Chroma vectorstore from documents\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    ids=ids,\n",
    "    collection_name=collection_name,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruebas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
