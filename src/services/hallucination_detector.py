#################################
##### ---    IMPORTS    --- #####
#################################

from abc import ABC, abstractmethod
from typing import List, Tuple, Union
from langchain_core.documents import Document

# import nltk
# try:
#     nltk.data.find('tokenizers/punkt')
# except LookupError:
#     print("Downloading NLTK punkt tokenizer")
#     nltk.download('punkt')

# LLM
import os
from dotenv import load_dotenv

load_dotenv()

# REF-CHECK
from refchecker import LLMChecker, LLMExtractor

#############################################
##### ---    CLASSES & FUNCTIONS    --- #####
#############################################

##### ---   ABSTRACT FACTORY CLASS   --- #####

class HallucinationDetector(ABC):
    """Abstract base class for hallucination detectors."""

    @abstractmethod
    def detect_hallucinations(
        self,
        response: str,
        context: Union[List[str], List[Document]],
        **kwargs
    ) -> List[Tuple[str, Union[str, float]]]:
        """
        Abstract method to detect hallucinations.

        Args:
            response (str): Generated response.
            context (Union[List[str], List[Document]]): Context or documents to evaluate against.
            kwargs: Additional arguments for specific implementations.

        Returns:
            List[Tuple[str, Union[str, float]]]: List of hallucinations and their scores/labels.
        """
        pass


##### ---   REF-CHECK DETECTOR CLASS   --- #####

class RefCheckerDetector(HallucinationDetector):
    def __init__(self, llm: str = 'gpt-4o'):
        self.extractor = LLMExtractor(model=llm)
        self.checker = LLMChecker(model=llm)

    def detect_hallucinations(
        self,
        response: str,
        context: Union[List[str], List[Document]],
        min_length: int = 3
    ) -> List[Tuple[str, str]]:
        """
        Detects hallucinations in the generated response using RefChecker.
        A sentence is classified as a hallucination if:
            - It has no 'Entailment' in any of the contexts.
            - It has both 'Contradiction' and 'Entailment' in the contexts.

        Args:
            response (str): The response generated by the model.
            context (Union[List[str], List[Document]]): The context used for evaluation, either as plain text or LangChain documents.
            min_length (int): Minimum token length for a sentence to be evaluated as a hallucination.

        Returns:
            List[Tuple[str, str]]: List of sentences detected as hallucinations along with their labels.
        """
        # Determine if the context is LangChain documents or plain text
        if isinstance(context[0], Document):
            context = [doc.page_content for doc in context]

        ## Proceed with senteces
        # # Split the response into sentences
        # sentences = nltk.sent_tokenize(response, language="english")

        # # Create batches of claims and references, filtering by min_length
        # batch_claims = [[sentence] for sentence in sentences if len(nltk.word_tokenize(sentence)) > min_length]
        # batch_references = [context] * len(batch_claims)
        
        # Proceed with triplets
        extraction_results = self.extractor.extract(
            batch_responses=[response],
            max_new_tokens=1000
        )

        batch_claims = [[c.content for c in res.claims] for  res in extraction_results]
        
        batch_references = [context] * len(batch_claims)

        # Verify the claims using RefChecker
        checking_results = self.checker.check(
            batch_claims=batch_claims,
            batch_references=batch_references,
            # max_reference_segment_length=200, # 100 - 300
            is_joint=False
        )
        
        hallucinations = []

        # Evaluate results and classify hallucinations
        for claims, results in zip(batch_claims, checking_results):
            for claim, result_list in zip(claims, results):
                # Check results for contradictions and entailments
                has_contradiction = "Contradiction" in result_list
            # print(f"Sentence: '{claim}' - Results: {result_list}")

            # Classify as hallucination if no entailment is found or both entailment and contradiction are present
            if has_contradiction:
                hallucinations.append((claim, result_list))

        return hallucinations


##### ---   HALLUCINATION DETECTOR FACTORY CLASS   --- #####

class HallucinationDetectorFactory:
    """
    Factory class that instantiates a Hallucination Detector based on the configuration.

    Supported Detector:
        - CrossEncoder: Requires 'model_name' (default: "all-mpnet-base-v2"; "jinaai/jina-reranker-v2-base-multilingual"(API) "microsoft/mpnet-base" for best performance), 'top_n' (default: 5).
        - LLM
        - SelfCheck
        - RAGAS: Requires 'llm'.
        - RefChecker: Requires 'llm'.
    """
    def load_detector(method_name: str, **kwargs) -> HallucinationDetector:
        """
        Factory method to instantiate and return the appropriate reranker based on the given method name.

        Args:
            method_name (str): The name of the reranking method to use.
            kwargs (dict): Additional keyword arguments to pass to the reranker class when creating an instance.

        Returns:
            Hallucination Detector: An instance of the specified Hallucination Detector class.

        Supported Detector:
            - CrossEncoder: Requires 'model_name' (default: "all-mpnet-base-v2"; "jinaai/jina-reranker-v2-base-multilingual"(API) "microsoft/mpnet-base" for best performance), 'top_n' (default: 5).
            - LLM
            - SelfCheck
            - RAGAS: Requires 'llm'.
            - RefChecker: Requires 'llm'.


        Raises:
        ValueError: If the method_name is not recognized.
        """
        detectors = {
            "RefChecker": RefCheckerDetector
        }

        if method_name not in detectors:
            raise ValueError(f"Unknown reranker method: {method_name}, implemented methods are: \n - CrossEncoder \n - LLM \n - SelfCheck \n - RAGAS \n - RefChecker \n - RankGPT \n - RankGPT_Threshold")

        # Instantiate the appropriate hallucination detector class with provided kwargs
        return detectors[method_name](**kwargs)
